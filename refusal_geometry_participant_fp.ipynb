{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# layers × models × precision for participancy ratio of given refusal direction calculed w diff o' means\n",
        "# PR = 1 means all variance in 1 dimension, PR = n means spread across n dimensions\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from transformer_lens import HookedTransformer\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from huggingface_hub import login as hf_login\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "hf_login(token=os.environ.get('HF_TOKEN'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = 'cuda'\n",
        "LLAMA_PATH = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "LAT_PATH = 'nlpett/llama-2-7b-chat-hf-LAT-layer4-hh'\n",
        "AT_PATH = 'nlpett/llama-2-7b-chat-hf-AT-hh'\n",
        "TEMPLATE = '\\n[INST]{prompt}[/INST]\\n'\n",
        "\n",
        "LAYERS = [4, 8, 12, 14, 16, 20]  # incl. LAT perturbation layer (4) and best ablation layer (14)\n",
        "PRECISIONS = [4, 8, 16]  # bits\n",
        "MODELS = [('BASE', None), ('AT', AT_PATH), ('LAT', LAT_PATH)]\n",
        "N = 416\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_quant_config(bits):\n",
        "    if bits == 4:\n",
        "        return BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16,\n",
        "                                   bnb_4bit_use_double_quant=True, bnb_4bit_quant_type='nf4'), torch.float16\n",
        "    elif bits == 8:\n",
        "        return BitsAndBytesConfig(load_in_8bit=True), torch.float16\n",
        "    else:\n",
        "        return None, torch.float16\n",
        "\n",
        "def clear_gpu():\n",
        "    gc.collect(); torch.cuda.empty_cache(); torch.cuda.synchronize()\n",
        "\n",
        "def load_model(peft_path, quant_config, dtype):\n",
        "    kwargs = {'torch_dtype': dtype, 'device_map': 'auto'}\n",
        "    if quant_config: kwargs['quantization_config'] = quant_config\n",
        "    base = AutoModelForCausalLM.from_pretrained(LLAMA_PATH, **kwargs)\n",
        "    if peft_path:\n",
        "        peft = PeftModel.from_pretrained(base, peft_path)\n",
        "        if not quant_config: peft = peft.to(dtype)\n",
        "        hf_model = peft.merge_and_unload(); hf_model.eval(); del peft\n",
        "    else:\n",
        "        hf_model = base\n",
        "    hooked = HookedTransformer.from_pretrained_no_processing(\n",
        "        LLAMA_PATH, hf_model=hf_model, dtype=dtype, device=DEVICE, default_padding_side='left')\n",
        "    hooked.tokenizer.padding_side = 'left'; hooked.tokenizer.pad_token = '[PAD]'\n",
        "    del base; clear_gpu()\n",
        "    return hooked\n",
        "\n",
        "def get_acts_multi_layer(model, instructions, layers, batch_size=16):\n",
        "    acts = {l: [] for l in layers}\n",
        "    for i in tqdm(range(0, len(instructions), batch_size)):\n",
        "        prompts = [TEMPLATE.format(prompt=p) for p in instructions[i:i+batch_size]]\n",
        "        toks = model.tokenizer(prompts, padding=True, return_tensors='pt').input_ids.to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            _, cache = model.run_with_cache(toks, names_filter=lambda n: 'resid_pre' in n)\n",
        "        for l in layers:\n",
        "            acts[l].append(cache['resid_pre', l][:, -1, :].cpu())\n",
        "        del cache\n",
        "    return {l: torch.cat(a, dim=0) for l, a in acts.items()}\n",
        "\n",
        "def participation_ratio(diffs):\n",
        "    diffs = diffs.float()\n",
        "    cov = (diffs.T @ diffs) / len(diffs)\n",
        "    eigs = torch.linalg.eigvalsh(cov)\n",
        "    eigs = eigs[eigs > 1e-10]\n",
        "    return (eigs.sum() ** 2 / (eigs ** 2).sum()).item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/data/advbench/harmful_behaviors.csv'\n",
        "harmful_train, _ = train_test_split(pd.read_csv(io.StringIO(requests.get(url).content.decode('utf-8')))['goal'].tolist(), \n",
        "                                     test_size=0.2, random_state=42)\n",
        "dataset = load_dataset('tatsu-lab/alpaca')\n",
        "harmless_all = [dataset['train'][i]['instruction'] for i in range(len(dataset['train'])) if dataset['train'][i]['input'].strip() == '']\n",
        "harmless_train, _ = train_test_split(harmless_all, test_size=0.2, random_state=42)\n",
        "print(f'Loaded {len(harmful_train)} harmful, {len(harmless_train)} harmless')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# main loop: precision -> model -> layers\n",
        "results = {}  # results[bits][model_name][layer] = PR value\n",
        "\n",
        "for bits in PRECISIONS:\n",
        "    quant_config, dtype = get_quant_config(bits)\n",
        "    results[bits] = {}\n",
        "    print(f'\\n{\"=\"*50}\\n{bits}-BIT\\n{\"=\"*50}')\n",
        "    \n",
        "    for model_name, peft_path in MODELS:\n",
        "        print(f'\\n--- {model_name} ---')\n",
        "        results[bits][model_name] = {}\n",
        "        \n",
        "        try:\n",
        "            model = load_model(peft_path, quant_config, dtype)\n",
        "            harmful_acts = get_acts_multi_layer(model, harmful_train[:N], LAYERS)\n",
        "            harmless_acts = get_acts_multi_layer(model, harmless_train[:N], LAYERS)\n",
        "            \n",
        "            for layer in LAYERS:\n",
        "                diffs = harmful_acts[layer] - harmless_acts[layer]\n",
        "                pr = participation_ratio(diffs)\n",
        "                results[bits][model_name][layer] = pr\n",
        "                print(f'  L{layer}: PR={pr:.1f}')\n",
        "            \n",
        "            del model, harmful_acts, harmless_acts\n",
        "            clear_gpu()\n",
        "        except Exception as e:\n",
        "            print(f'ERROR: {e}')\n",
        "            clear_gpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\nParticipation Ratio (effective dimensions) at Layer 14:')\n",
        "print(f\"{'Precision':<10} {'BASE':>8} {'AT':>8} {'LAT':>8}\")\n",
        "print('-'*36)\n",
        "for bits in PRECISIONS:\n",
        "    row = f\"{bits}-bit{'':<5}\"\n",
        "    for m in ['BASE', 'AT', 'LAT']:\n",
        "        val = results[bits].get(m, {}).get(14, None)\n",
        "        row += f\"{val:>8.1f}\" if val else f\"{'N/A':>8}\"\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot: PR across layers for each model, faceted by precision\n",
        "fig, axes = plt.subplots(1, len(PRECISIONS), figsize=(4*len(PRECISIONS), 4), sharey=True)\n",
        "colors = {'BASE': 'tab:blue', 'AT': 'tab:orange', 'LAT': 'tab:red'}\n",
        "\n",
        "for ax, bits in zip(axes, PRECISIONS):\n",
        "    for model_name in ['BASE', 'AT', 'LAT']:\n",
        "        if model_name in results[bits]:\n",
        "            prs = [results[bits][model_name].get(l, np.nan) for l in LAYERS]\n",
        "            ax.plot(LAYERS, prs, 'o-', label=model_name, color=colors[model_name])\n",
        "    ax.set_xlabel('Layer')\n",
        "    ax.set_title(f'{bits}-bit')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_xticks(LAYERS)\n",
        "\n",
        "axes[0].set_ylabel('Participation Ratio')\n",
        "axes[-1].legend()\n",
        "fig.suptitle('Participation Ratio (Effective Dimensions) by Layer', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot: PR at layer 14 vs precision\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "prec_labels = [f'{b}-bit' for b in PRECISIONS]\n",
        "\n",
        "for model_name in ['BASE', 'AT', 'LAT']:\n",
        "    prs = [results[b].get(model_name, {}).get(14, np.nan) for b in PRECISIONS]\n",
        "    ax.plot(prec_labels, prs, 'o-', label=model_name, color=colors[model_name], linewidth=2, markersize=8)\n",
        "\n",
        "ax.set_xlabel('Precision')\n",
        "ax.set_ylabel('Participation Ratio (effective dims)')\n",
        "ax.set_title('Precision effects on observed dimensionality of the refusal direction at L14')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
