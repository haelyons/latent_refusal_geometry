{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test: does 4-bit quantization reproduce paper results\n",
        "# Paper: BASE ~49%, AT ~43%, LAT ~51% top-1 variance\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "import gc\n",
        "from scipy.linalg import svd\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from transformer_lens import HookedTransformer\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from huggingface_hub import login as hf_login\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "hf_login(token=os.environ.get('HF_TOKEN'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = 'cuda'\n",
        "DTYPE = torch.float16\n",
        "LLAMA_PATH = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "LAT_PATH = 'nlpett/llama-2-7b-chat-hf-LAT-layer4-hh'\n",
        "AT_PATH = 'nlpett/llama-2-7b-chat-hf-AT-hh'\n",
        "TEMPLATE = '\\n[INST]{prompt}[/INST]\\n'\n",
        "\n",
        "# 4-bit config from original paper code\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=DTYPE,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type='nf4'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_4bit(peft_path=None, name='base'):\n",
        "    base = AutoModelForCausalLM.from_pretrained(\n",
        "        LLAMA_PATH, torch_dtype=DTYPE, device_map='cuda:0', quantization_config=bnb_config\n",
        "    )\n",
        "    if peft_path:\n",
        "        peft = PeftModel.from_pretrained(base, peft_path).to(DTYPE)\n",
        "        merged = peft.merge_and_unload()\n",
        "        merged.eval()\n",
        "        hf_model = merged\n",
        "    else:\n",
        "        hf_model = base\n",
        "    \n",
        "    hooked = HookedTransformer.from_pretrained_no_processing(\n",
        "        LLAMA_PATH, hf_model=hf_model, dtype=DTYPE, device=DEVICE, default_padding_side='left'\n",
        "    )\n",
        "    hooked.tokenizer.padding_side = 'left'\n",
        "    hooked.tokenizer.pad_token = '[PAD]'\n",
        "    \n",
        "    del base\n",
        "    if peft_path:\n",
        "        del peft, merged\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    return hooked\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load data\n",
        "url = 'https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/data/advbench/harmful_behaviors.csv'\n",
        "harmful_train, _ = train_test_split(\n",
        "    pd.read_csv(io.StringIO(requests.get(url).content.decode('utf-8')))['goal'].tolist(), \n",
        "    test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "dataset = load_dataset('tatsu-lab/alpaca')\n",
        "harmless_all = [dataset['train'][i]['instruction'] for i in range(len(dataset['train'])) \n",
        "                if dataset['train'][i]['input'].strip() == '']\n",
        "harmless_train, _ = train_test_split(harmless_all, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'harmful: {len(harmful_train)}, harmless: {len(harmless_train)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_acts(model, instructions, layer=14, batch_size=16):\n",
        "    acts = []\n",
        "    for i in tqdm(range(0, len(instructions), batch_size)):\n",
        "        batch = instructions[i:i+batch_size]\n",
        "        prompts = [TEMPLATE.format(prompt=p) for p in batch]\n",
        "        toks = model.tokenizer(prompts, padding=True, return_tensors='pt').input_ids.to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            _, cache = model.run_with_cache(toks, names_filter=lambda n: f'blocks.{layer}.hook_resid_pre' in n)\n",
        "        acts.append(cache[f'blocks.{layer}.hook_resid_pre'][:, -1, :])\n",
        "    return torch.cat(acts, dim=0)\n",
        "\n",
        "def svd_variance(diffs):\n",
        "    _, s, _ = svd(diffs.cpu().numpy(), full_matrices=False)\n",
        "    v = (s**2) / np.sum(s**2)\n",
        "    return v[0], sum(v[:2]), sum(v[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "N = 416\n",
        "results = {}\n",
        "\n",
        "for name, peft_path in [('BASE', None), ('AT', AT_PATH), ('LAT', LAT_PATH)]:\n",
        "    print(f'\\n=== {name} ===')\n",
        "    model = load_model_4bit(peft_path, name)\n",
        "    \n",
        "    harmful_acts = get_acts(model, harmful_train[:N])\n",
        "    harmless_acts = get_acts(model, harmless_train[:N])\n",
        "    diffs = harmful_acts - harmless_acts\n",
        "    \n",
        "    top1, top2, top10 = svd_variance(diffs)\n",
        "    results[name] = {'top1': top1, 'top2': top2, 'top10': top10}\n",
        "    print(f'{name}: top-1={top1*100:.1f}%, top-2={top2*100:.1f}%, top-10={top10*100:.1f}%')\n",
        "    \n",
        "    del model, harmful_acts, harmless_acts, diffs\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"{'Model':<8} {'4bit top-1':>12} {'Paper top-1':>12} {'Match?':>10}\")\n",
        "print('-' * 44)\n",
        "paper = {'BASE': 49, 'AT': 43, 'LAT': 51}\n",
        "for name in ['BASE', 'AT', 'LAT']:\n",
        "    got = results[name]['top1']*100\n",
        "    expected = paper[name]\n",
        "    match = '✓' if abs(got - expected) < 10 else '✗'\n",
        "    print(f\"{name:<8} {got:>11.1f}% {expected:>11}% {match:>10}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
